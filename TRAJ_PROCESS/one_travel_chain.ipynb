{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from shapely.geometry import Point\n",
    "import geopandas as gpd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Add grid id and merge nan rows"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('../data/one_travel_chain_origin.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\liboy\\anaconda3\\envs\\jax\\lib\\site-packages\\pandas\\core\\dtypes\\cast.py:126: ShapelyDeprecationWarning: The array interface is deprecated and will no longer work in Shapely 2.0. Convert the '.coords' to a numpy array instead.\n",
      "  arr = construct_1d_object_array_from_listlike(values)\n",
      "c:\\Users\\liboy\\anaconda3\\envs\\jax\\lib\\site-packages\\pandas\\core\\dtypes\\cast.py:126: ShapelyDeprecationWarning: The array interface is deprecated and will no longer work in Shapely 2.0. Convert the '.coords' to a numpy array instead.\n",
      "  arr = construct_1d_object_array_from_listlike(values)\n"
     ]
    }
   ],
   "source": [
    "df['geometry_o'] = [Point(xy) for xy in zip(df['lambda_o'], df['phi_o'])]\n",
    "df['geometry_d'] = [Point(xy) for xy in zip(df['lambda_d'], df['phi_d'])]\n",
    "grid_gdf = gpd.read_file(\"../data/shenzhen_grid/shenzhen_grid.shp\")\n",
    "\n",
    "geo_df_o = gpd.GeoDataFrame(df, geometry='geometry_o')\n",
    "geo_df_d = gpd.GeoDataFrame(df, geometry='geometry_d')\n",
    "\n",
    "# Ensure CRS matches before joining\n",
    "geo_df_o.set_crs(grid_gdf.crs, inplace=True)\n",
    "geo_df_d.set_crs(grid_gdf.crs, inplace=True)\n",
    "\n",
    "result_o = gpd.sjoin(geo_df_o, grid_gdf, how=\"left\", predicate=\"within\")\n",
    "result_d = gpd.sjoin(geo_df_d, grid_gdf, how=\"left\", predicate=\"within\")\n",
    "\n",
    "df['grid_id_o'] = result_o['fnid']  \n",
    "df['grid_id_d'] = result_d['fnid']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## merging trajectory breakpoints caused by leaving the city"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index([  51,   52,   77,   78,   96,   97,  154,  155,  168,  169,  515,  516,\n",
       "        798,  799,  805,  806, 1017, 1018, 1171, 1172, 1312, 1313, 1314, 1315],\n",
       "      dtype='int64')"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Finding rows with at least one NaN value, out of city\n",
    "columns_to_check = [\"lambda_o\",\"phi_o\",\"lambda_d\",\"phi_d\",\"grid_id_o\",\"grid_id_d\"]\n",
    "rows_with_nan = df[columns_to_check].isna().any(axis=1)\n",
    "nan_indexes = df[rows_with_nan].index\n",
    "nan_indexes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in nan_indexes[::2]:\n",
    "    df.at[i, 'etime'] = df.at[i + 1, 'etime']\n",
    "    df.iloc[i] = df.iloc[i].combine_first(df.iloc[i + 1])\n",
    "df.drop(nan_indexes[1::2], inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.fillna(0, inplace=True)\n",
    "df.reset_index(drop=True, inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## merging trajectory breakpoints caused by midnight"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[69, 113, 216, 338, 465, 783, 980, 1143, 1163, 1303, 1304, 1317, 1486]\n"
     ]
    }
   ],
   "source": [
    "discontinuous_rows = []\n",
    "\n",
    "for i in range(len(df) - 1):\n",
    "    if df['lambda_d'].iloc[i] != df['lambda_o'].iloc[i + 1]:\n",
    "        # next row index\n",
    "        discontinuous_rows.append(i + 1)\n",
    "print(discontinuous_rows)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "for idx in discontinuous_rows:\n",
    "    df.loc[idx, 'poi_o'] = df.loc[idx - 1, 'poi_d']\n",
    "    df.loc[idx, 'org_chess_x'] = df.loc[idx - 1, 'dst_chess_x']\n",
    "    df.loc[idx, 'org_chess_y'] = df.loc[idx - 1, 'dst_chess_y']\n",
    "    df.loc[idx, 'lambda_o'] = df.loc[idx - 1, 'lambda_d']\n",
    "    df.loc[idx, 'phi_o'] = df.loc[idx - 1, 'phi_d']\n",
    "    df.loc[idx, 'pre_chess_x'] = df.loc[idx - 1, 'post_chess_x']\n",
    "    df.loc[idx, 'pre_chess_y'] = df.loc[idx - 1, 'post_chess_y']\n",
    "    df.loc[idx, 'grid_id_o'] = df.loc[idx-1, 'grid_id_d']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[]\n"
     ]
    }
   ],
   "source": [
    "discontinuous_rows = []\n",
    "for i in range(len(df) - 1):\n",
    "    if df['grid_id_d'].iloc[i] != df['grid_id_o'].iloc[i + 1]:\n",
    "        # next row index\n",
    "        discontinuous_rows.append(i + 1)\n",
    "print(discontinuous_rows)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Drop columns and save to feature csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "columns_to_drop = [\n",
    "    'poi_o','poi_d','org_chess_x', 'org_chess_y', 'dst_chess_x',\n",
    "    'dst_chess_y', 'pre_chess_x', 'pre_chess_y', 'post_chess_x', 'post_chess_y',\n",
    "    'geometry_o', 'geometry_d','home_distance','trip_distance'\n",
    "]\n",
    "\n",
    "df = df.drop(columns=columns_to_drop)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.to_csv('../data/one_travel_chain.csv',index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DataFrame with 'date' before 'migrt':\n",
      "627\n",
      "DataFrame with 'date' after 'migrt':\n",
      "1043\n"
     ]
    }
   ],
   "source": [
    "df_before_migrt = df[df['date'] < df['migrt']]\n",
    "df_after_migrt = df[df['date']>df['migrt']]\n",
    "\n",
    "print(\"DataFrame with 'date' before 'migrt':\")\n",
    "print(len(df_before_migrt))\n",
    "print(\"DataFrame with 'date' after 'migrt':\")\n",
    "print(len(df_after_migrt))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_before_migrt.to_csv('../data/one_travel_before.csv',index=False)\n",
    "df_after_migrt.to_csv('../data/one_travel_after.csv',index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mergeFeatureDataframes(df):\n",
    "\n",
    "    selectedColumns1 = ['grid_id_d', 'pre_home_distance', 'post_home_distance', 'LU_Business',\n",
    "                        'LU_City_Road', 'LU_Consumption', 'LU_Culture', 'LU_Industry',\n",
    "                        'LU_Medical', 'LU_Park_&_Scenery', 'LU_Public', 'LU_Residence',\n",
    "                        'LU_Science_&_Education', 'LU_Special', 'LU_Transportation', 'LU_Wild'\n",
    "                        ]\n",
    "    \n",
    "    featureDf1 = df[selectedColumns1].drop_duplicates(subset=['grid_id_d'])\n",
    "    featureDf1 = featureDf1.rename(columns={'grid_id_d':'fnid'})\n",
    "\n",
    "    selectedColumns2 = ['grid_id_o', 'LU_Business',\n",
    "                        'LU_City_Road', 'LU_Consumption', 'LU_Culture', 'LU_Industry',\n",
    "                        'LU_Medical', 'LU_Park_&_Scenery', 'LU_Public', 'LU_Residence',\n",
    "                        'LU_Science_&_Education', 'LU_Special', 'LU_Transportation', 'LU_Wild'\n",
    "                        ]\n",
    "\n",
    "    featureDf2 = df[selectedColumns2].drop_duplicates(subset=['grid_id_o'])\n",
    "    featureDf2 = featureDf2.rename(columns={'grid_id_o':'fnid'})\n",
    "\n",
    "    mergedDf = featureDf1.merge(featureDf2, on='fnid', how='outer')\n",
    "    mergedDf = mergedDf.fillna(0)\n",
    "\n",
    "    for col in selectedColumns2:\n",
    "        if col != 'fnid' and col in featureDf1.columns and col in featureDf2.columns:\n",
    "            mergedDf[col] = mergedDf[col + '_x'] + mergedDf[col + '_y']\n",
    "            mergedDf.drop([col + '_x', col + '_y'], axis=1, inplace=True)\n",
    "\n",
    "    mergedDf.fillna(0, inplace=True)\n",
    "    return mergedDf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "before_feature=mergeFeatureDataframes(df_before_migrt)\n",
    "before_feature.to_csv('../data/before_migrt_feature.csv',index=False)\n",
    "\n",
    "after_feature = mergeFeatureDataframes(df_after_migrt)\n",
    "after_feature.to_csv('../data/after_migrt_feature.csv',index=False)\n",
    "\n",
    "all_feature = mergeFeatureDataframes(df)\n",
    "all_feature.to_csv('../data/all_traj_feature.csv',index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "jax",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
